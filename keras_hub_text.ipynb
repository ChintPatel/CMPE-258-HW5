{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOCGu4FqmrsAcicZf4jCorg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChintPatel/CMPE-258-HW5/blob/main/keras_hub_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx4ELCvCHI3b",
        "outputId": "4bf1761d-0765-4ce6-8f3b-4228195ad893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Easy: IMDB Sentiment Analysis with GRU\n",
        "Variation: Use GRU instead of LSTM."
      ],
      "metadata": {
        "id": "8phEmGM2HqhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess IMDB data\n",
        "vocab_size = 10000\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "max_length = 200\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_length)\n",
        "\n",
        "# Build model with GRU\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, 128, input_length=max_length),\n",
        "    keras.layers.GRU(128),  # Variation\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile and train\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=3, batch_size=64, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate\n",
        "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Easy - Test accuracy: {acc:.4f}\")\n",
        "# Explanation: GRU is lighter than LSTM, testing efficiency vs. performance trade-off."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw7VBJmdHoWY",
        "outputId": "53c2ea8e-e266-48bc-e2e5-73730a9196d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.6505 - loss: 0.6006 - val_accuracy: 0.8358 - val_loss: 0.3780\n",
            "Epoch 2/3\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.8867 - loss: 0.2883 - val_accuracy: 0.8632 - val_loss: 0.3220\n",
            "Epoch 3/3\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.9334 - loss: 0.1832 - val_accuracy: 0.8760 - val_loss: 0.3357\n",
            "Easy - Test accuracy: 0.8656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intermediate: Reuters News Classification with Conv1D\n",
        "Variation: Add a convolutional layer for local pattern detection."
      ],
      "metadata": {
        "id": "LMSNKhe6HtZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess Reuters data\n",
        "vocab_size = 10000\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.reuters.load_data(num_words=vocab_size)\n",
        "max_length = 200\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_length)\n",
        "num_classes = np.max(y_train) + 1\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Build model with Conv1D\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, 128, input_length=max_length),\n",
        "    keras.layers.Conv1D(64, 5, activation='relu'),  # Variation\n",
        "    keras.layers.GlobalMaxPooling1D(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile and train\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate\n",
        "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Intermediate - Test accuracy: {acc:.4f}\")\n",
        "# Explanation: Conv1D captures local word patterns, enhancing multi-class accuracy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4thK5uUxH406",
        "outputId": "aec547c0-91d6-4a99-e1bc-00f172449f02"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "\u001b[1m2110848/2110848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.4321 - loss: 2.4859 - val_accuracy: 0.6416 - val_loss: 1.5080\n",
            "Epoch 2/5\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6898 - loss: 1.2988 - val_accuracy: 0.7329 - val_loss: 1.1623\n",
            "Epoch 3/5\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7909 - loss: 0.8828 - val_accuracy: 0.7602 - val_loss: 1.0328\n",
            "Epoch 4/5\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8624 - loss: 0.5384 - val_accuracy: 0.7685 - val_loss: 1.0245\n",
            "Epoch 5/5\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9183 - loss: 0.3395 - val_accuracy: 0.7718 - val_loss: 1.0723\n",
            "Intermediate - Test accuracy: 0.7703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced: Bidirectional LSTM for IMDB with More Units\n",
        "Variation: Increase LSTM units for deeper learning."
      ],
      "metadata": {
        "id": "EVTFPxIAH6zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess IMDB data\n",
        "vocab_size = 10000\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "max_length = 200\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_length)\n",
        "\n",
        "# Build model with more units\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, 128, input_length=max_length),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(128)),  # Variation: 128 units\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile and train\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=3, batch_size=64, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate\n",
        "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Advanced - Test accuracy: {acc:.4f}\")\n",
        "# Explanation: More units in Bidirectional LSTM capture complex sequence patterns better."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U38LCL2MH9AW",
        "outputId": "0c23d45b-73d8-4d20-dfe0-0a3a64365b78"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.6976 - loss: 0.5541 - val_accuracy: 0.8524 - val_loss: 0.3560\n",
            "Epoch 2/3\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.8861 - loss: 0.2882 - val_accuracy: 0.8554 - val_loss: 0.3528\n",
            "Epoch 3/3\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.9181 - loss: 0.2191 - val_accuracy: 0.8266 - val_loss: 0.3824\n",
            "Advanced - Test accuracy: 0.8259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expert: Text Generation with Stacked LSTM\n",
        "Variation: Use two LSTM layers for deeper modeling."
      ],
      "metadata": {
        "id": "OENlaChpH-l_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a small synthetic dataset for speed\n",
        "text = \"the quick brown fox jumps over the lazy dog \" * 100\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
        "max_length = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - max_length, step):\n",
        "    sentences.append(text[i:i + max_length])\n",
        "    next_chars.append(text[i + max_length])\n",
        "x = np.zeros((len(sentences), max_length, len(chars)), dtype=bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_to_idx[char]] = 1\n",
        "    y[i, char_to_idx[next_chars[i]]] = 1\n",
        "\n",
        "# Build model with stacked LSTM\n",
        "model = keras.Sequential([\n",
        "    keras.layers.LSTM(128, input_shape=(max_length, len(chars)), return_sequences=True),  # Variation\n",
        "    keras.layers.LSTM(128),\n",
        "    keras.layers.Dense(len(chars), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile and train\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "model.fit(x, y, epochs=5, batch_size=128, verbose=1)\n",
        "\n",
        "# Generate text\n",
        "seed = \"the quick brown fox jumps over the laz\"\n",
        "generated = seed\n",
        "for _ in range(50):\n",
        "    x_pred = np.zeros((1, max_length, len(chars)))\n",
        "    for t, char in enumerate(seed[-max_length:]):\n",
        "        x_pred[0, t, char_to_idx[char]] = 1\n",
        "    preds = model.predict(x_pred, verbose=0)[0]\n",
        "    next_idx = np.argmax(preds)\n",
        "    next_char = idx_to_char[next_idx]\n",
        "    generated += next_char\n",
        "    seed = seed[1:] + next_char\n",
        "print(f\"Generated text: {generated}\")\n",
        "# Explanation: Stacked LSTMs model deeper dependencies, improving text coherence."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXpvYPn0IBFv",
        "outputId": "1646cb57-876a-477b-b297-8459240990c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 3.2412\n",
            "Epoch 2/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0097\n",
            "Epoch 3/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.6666\n",
            "Epoch 4/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.1529\n",
            "Epoch 5/5\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.7828\n",
            "Generated text: the quick brown fox jumps over the laz gg  e  qii   bwwwnooo  ummpppo     llzaydog  te q\n"
          ]
        }
      ]
    }
  ]
}